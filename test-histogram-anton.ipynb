{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-14T11:06:53.447578Z","iopub.execute_input":"2023-12-14T11:06:53.448532Z","iopub.status.idle":"2023-12-14T11:06:53.453917Z","shell.execute_reply.started":"2023-12-14T11:06:53.448494Z","shell.execute_reply":"2023-12-14T11:06:53.453050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U py-boost","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:35:03.113850Z","iopub.execute_input":"2023-12-14T11:35:03.114213Z","iopub.status.idle":"2023-12-14T11:35:15.063917Z","shell.execute_reply.started":"2023-12-14T11:35:03.114182Z","shell.execute_reply":"2023-12-14T11:35:15.062893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy as cp\nfrom py_boost.gpu.utils import *\nfrom astropy.table import Table\nfrom cupyx.profiler import benchmark","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:35:31.978068Z","iopub.execute_input":"2023-12-14T11:35:31.978419Z","iopub.status.idle":"2023-12-14T11:35:31.983253Z","shell.execute_reply.started":"2023-12-14T11:35:31.978393Z","shell.execute_reply":"2023-12-14T11:35:31.982091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histogram_kernel_idx_elw = cp.ElementwiseKernel(\n    \"\"\"\n    uint64 i_, uint64 j_, uint64 k_,\n    uint64 kk,\n\n    raw uint64 jj,\n    raw bool padded_bool_indexer,\n\n    raw float32 target,\n    raw T arr,\n    raw int32 nodes,\n\n    uint64 hlen,\n    uint64 flen,\n    uint64 length,\n    uint64 feats,\n    uint64 nout\n    \"\"\",\n    'raw float32 hist',\n\n    \"\"\"\n    unsigned int feat_4t = arr[i_ * feats + j_];\n    int d;\n    int j;\n    int val;\n    int pos;\n    float *x_ptr;\n    float y = target[i_ * nout + k_];\n\n    for (d = 0; d < 4; d++) {\n\n        pos = (i_ + d) % 4;\n\n        if (padded_bool_indexer[j_ * 4 + pos]) {\n\n            val = (feat_4t >> (8 * pos)) % 256;\n            j = jj[j_ * 4 + pos];\n            x_ptr = &hist[0] +  kk * hlen + nodes[i_] * flen + j * length + val;\n            atomicAdd(x_ptr, y);\n        }\n    }\n\n    \"\"\",\n\n    'histogram_kernel_idx')","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:35:37.479231Z","iopub.execute_input":"2023-12-14T11:35:37.480088Z","iopub.status.idle":"2023-12-14T11:35:37.485274Z","shell.execute_reply.started":"2023-12-14T11:35:37.480054Z","shell.execute_reply":"2023-12-14T11:35:37.484267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histogram_kernel_idx_ser = cp.ElementwiseKernel(\n    \"\"\"\n    uint64 i_, uint64 j_, uint64 k_,\n    uint64 kk,\n\n    raw uint64 jj,\n    raw bool padded_bool_indexer,\n\n    raw float32 target,\n    raw T arr,\n    raw int32 nodes,\n\n    uint64 hlen,\n    uint64 flen,\n    uint64 length,\n    uint64 feats,\n    uint64 nout\n    \"\"\",\n    'raw float32 hist',\n\n    \"\"\"\n    unsigned int feat_4t = arr[i_ * feats + j_];\n\n    unsigned long long j;\n    unsigned int val;\n    unsigned char pos;\n    float y = target[i_ * nout + k_];\n\n    unsigned long long posj = j_ * 4;\n\n    unsigned long long posk = kk * hlen + nodes[i_] * flen;\n\n    for (unsigned char d = 0; d < 4; ++d) {\n\n        pos = (i_ + d) % 4;\n\n        if (padded_bool_indexer[posj + pos]) {\n\n            val = (feat_4t >> (8 * pos)) % 256;\n            j = jj[posj + pos];\n            atomicAdd(&hist[posk + j * length  + val], y);\n        }\n    }\n\n    \"\"\",\n\n    'histogram_kernel_idx_ser')","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:35:41.404946Z","iopub.execute_input":"2023-12-14T11:35:41.405919Z","iopub.status.idle":"2023-12-14T11:35:41.411667Z","shell.execute_reply.started":"2023-12-14T11:35:41.405882Z","shell.execute_reply":"2023-12-14T11:35:41.410554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_histogram_tmp(res, arr, target, nodes, col_indexer, row_indexer, out_indexer, func='elw'):\n    \"\"\"Fill the histogram res\n\n    Args:\n        res: cp.ndarray, histogram of zeros, shape (n_out, n_nodes, n_features, n_bins)\n        arr: cp.ndarray, features array, shape (n_data, n_features)\n        target: cp.ndarray, values to accumulate, shape (n_data, n_out)\n        nodes: cp.ndarray, tree node indices, shape (n_data, )\n        col_indexer: cp.ndarray, indices of features to accumulate\n        row_indexer: cp.ndarray, indices of rows to accumulate\n        out_indexer: cp.ndarray, indices of outputs to accumulate\n        func: numeric flag to choose the kernel that will be used in histogram calculations\n\n    Returns:\n\n    \"\"\"\n    # define data split for kernel launch\n    nout, nnodes, nfeats, nbins = res.shape\n\n    # padded array of 4 feature tuple\n    arr_4t = arr.base.view(dtype=cp.uint32)\n    pfeats = arr_4t.shape[1]\n\n    # create 4 feats tuple indexer\n    padded_bool_indexer = cp.zeros((arr.base.shape[1],), dtype=cp.bool_)\n    padded_col_indexer = cp.zeros((arr.base.shape[1],), dtype=cp.uint64)\n    tuple_indexer = cp.zeros((arr_4t.shape[1],), dtype=cp.bool_)\n\n    feature_grouper_kernel(col_indexer, padded_bool_indexer, tuple_indexer, padded_col_indexer)\n    tuple_indexer = cp.arange(arr_4t.shape[1], dtype=cp.uint64)[tuple_indexer]\n\n    fb = nfeats * nbins\n    nfb = nnodes * fb\n\n    magic_constant = 2 ** 19  # optimal value for my V100\n\n    # split features\n    nsplits = math.ceil(nfb / magic_constant)\n    # first split by feats\n    feats_batch = math.ceil(pfeats / nsplits)\n    # split by features\n    if feats_batch == nfeats:\n        out_batch = magic_constant // nfb\n    else:\n        out_batch = 1\n\n    ri = row_indexer[:, cp.newaxis, cp.newaxis]\n    ti = tuple_indexer[cp.newaxis, :, cp.newaxis]\n    oi = out_indexer[cp.newaxis, cp.newaxis, :]\n    \n    nrows = ri.shape[0]\n\n    oii = cp.arange(oi.shape[2], dtype=cp.uint64)[cp.newaxis, cp.newaxis, :]\n    \n    if func == 'ser2' or func == 'ser3':\n        with cp.cuda.Device(0):\n            res0 = cp.zeros(res.shape, dtype=cp.float32)\n        with cp.cuda.Device(1):\n            res1 = cp.zeros(res.shape, dtype=cp.float32)\n            ri_d1 = ri.copy()\n            padded_col_indexer_d1  = padded_col_indexer.copy()\n            padded_bool_indexer_d1 = padded_bool_indexer.copy()\n            target_d1 = target.copy()\n            arr_4t_d1 = arr_4t.copy()\n            nodes_d1  = nodes.copy()\n            nfb_d1    = nfb\n            fb_d1     = fb\n            nbins_d1  = nbins\n            nout_d1   = nout\n            \n    for j in range(0, pfeats, feats_batch):\n        ti_ = ti[:, j: j + feats_batch]\n\n        for k in range(0, nout, out_batch):\n            oi_ = oi[..., k: k + out_batch]\n            oii_ = oii[..., k: k + out_batch]\n\n            if func == 'elw':\n                # Use original Anton's solution\n                histogram_kernel_idx_elw(ri, ti_, oi_,\n                                     oii_,\n                                     padded_col_indexer,\n                                     padded_bool_indexer,\n                                     target,\n                                     arr_4t,\n                                     nodes,\n                                     nfb, fb, nbins, arr_4t.shape[1], nout,\n                                     res, block_size=1024)\n            if func == 'ser':\n                histogram_kernel_idx_ser(ri, ti_, oi_,\n                                         oii_,\n                                         padded_col_indexer,\n                                         padded_bool_indexer,\n                                         target,\n                                         arr_4t,\n                                         nodes,\n                                         nfb, fb, nbins, arr_4t.shape[1], nout,\n                                         res, block_size=1024 )\n            if func == 'ser2':\n                with cp.cuda.Device(0):\n                    histogram_kernel_idx_ser(ri[0:nrows//2], ti_, oi_,\n                                             oii_,\n                                             padded_col_indexer,\n                                             padded_bool_indexer,\n                                             target,\n                                             arr_4t,\n                                             nodes,\n                                             nfb, fb, nbins, arr_4t.shape[1], nout,\n                                             res0, block_size=1024 )\n\n                with cp.cuda.Device(1):\n                    ti_d1  = ti_.copy()\n                    oi_d1  = oi_.copy()\n                    oii_d1 = oii_.copy()\n                    histogram_kernel_idx_ser(ri_d1[nrows//2:], ti_d1, oi_d1,\n                                             oii_d1,\n                                             padded_col_indexer_d1,\n                                             padded_bool_indexer_d1,\n                                             target_d1,\n                                             arr_4t_d1,\n                                             nodes_d1,\n                                             nfb_d1, fb_d1, nbins_d1, arr_4t_d1.shape[1], nout_d1,\n                                             res1, block_size=1024 )\n            if func == 'ser2_target':\n                with cp.cuda.Device(0):\n                    histogram_kernel_idx_ser(ri, ti_, oi_,\n                                             oii_,\n                                             padded_col_indexer,\n                                             padded_bool_indexer,\n                                             target[0:nrows//2],\n                                             arr_4t,\n                                             nodes,\n                                             nfb, fb, nbins, arr_4t.shape[1], nout,\n                                             res, block_size=1024 )\n                with cp.cuda.Device(1):\n                    ti_d1  = ti_.copy()\n                    oi_d1  = oi_.copy()\n                    oii_d1 = oii_.copy()\n                    histogram_kernel_idx_ser(ri_d1, ti_d1, oi_d1,\n                                             oii_d1,\n                                             padded_col_indexer_d1,\n                                             padded_bool_indexer_d1,\n                                             target_d1[nrows//2:],\n                                             arr_4t_d1,\n                                             nodes_d1,\n                                             nfb_d1, fb_d1, nbins_d1, arr_4t_d1.shape[1], nout_d1,\n                                             res1, block_size=1024)\n                    \n    if func == 'ser2' or func == 'ser2_target':\n        with cp.cuda.Device(0):\n            res[:] = res0 + res1\n            \n    return \n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:08:51.748990Z","iopub.execute_input":"2023-12-14T12:08:51.749369Z","iopub.status.idle":"2023-12-14T12:08:51.774028Z","shell.execute_reply.started":"2023-12-14T12:08:51.749343Z","shell.execute_reply":"2023-12-14T12:08:51.773085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_idx(n, sample):\n    # THIST FUNCTION GENERATES IDS USED \n    # IN THE HISTOGRAM CALCULATIONS\n    \n    idx = cp.arange(n, dtype=cp.uint64)\n    sl = cp.random.rand(n) < sample\n    \n    return cp.ascontiguousarray(idx[sl])\n\ndef generate_input( n_rows, n_cols, n_out, max_bin, nnodes, \n                    colsample=0.8, subsample=0.8, outsample=1.0, verbose=False, seed=42):\n    # THIS FUNCTION GENERATES ALL INPUT\n    # ARRAYS, REQUIRED BY THE HISTOGRAM \n    # FUNCTION IN PY-BOOST\n    # Input: \n    # n_rows   - number of rows in the input array\n    # n_cols   - number of cols in the input array\n    # n_out    - number of ???? in the output array\n    # max_bins - number of histogram bins (can't be >256 really)\n    # nnodes   - ????\n    \n    np.random.seed(seed)\n    features_cpu = np.random.randint(0, max_bin, size=(n_rows, n_cols)).astype(np.uint8)\n    features_gpu = pad_and_move(features_cpu)\n    cp.random.seed(seed)\n    targets_gpu  = cp.random.rand(n_rows, n_out).astype(np.float32)\n    cp.random.seed(seed)\n    nodes_gpu    = cp.random.randint(0, nnodes, size=(n_rows, )).astype(np.int32)\n    cp.random.seed(seed)\n    \n    if verbose == True:\n        print('Initial CPU features shape: {}'.format(features_cpu.shape))\n        print('Padded  GPU features shape: {}'.format(features_gpu.shape))\n        print('Nodes   GPU vector   shape: {}'.format(nodes_gpu.shape   ))\n        print('Targets GPU array    shape: {}'.format(targets_gpu.shape ))\n    \n    row_indexer = sample_idx(n_rows, subsample)\n    col_indexer = sample_idx(n_cols, colsample)\n    out_indexer = sample_idx(n_out, outsample)\n    \n    if verbose == True:\n        print('Sampled rows shape:    {}'.format(row_indexer.shape))\n        print('Sampled columns shape: {}'.format(col_indexer.shape))\n        print('Sampled output shape:  {}'.format(out_indexer.shape))\n    \n    nout   = out_indexer.shape[0]\n    nfeats = col_indexer.shape[0]\n    \n    # Anton's function takes the following input arguments + the empty array to \n    # store the resulting histogram bins (comes in the first position)\n    # input: res, X, Y, nodes, col_indexer, row_indexer, out_indexer\n    \n    res    = cp.zeros((nout, nnodes, nfeats, max_bin), dtype=cp.float32)\n    params = (res, features_gpu, targets_gpu, nodes_gpu, col_indexer, row_indexer, out_indexer)\n    \n    if verbose == True:\n        true_res = nfeats * targets_gpu[row_indexer].sum()\n        print ('Sum of the resulting histogram must be {} ({}/2={})'.format( true_res, true_res, true_res/2 ))\n    return params","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:36:10.162925Z","iopub.execute_input":"2023-12-14T11:36:10.163287Z","iopub.status.idle":"2023-12-14T11:36:10.176130Z","shell.execute_reply.started":"2023-12-14T11:36:10.163256Z","shell.execute_reply":"2023-12-14T11:36:10.175208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original Anton's code on 1 GPU\ninput_params = generate_input(n_rows=pow(10,6),n_cols=99,n_out=10,max_bin=256,nnodes=32,verbose=False)\ntau0 = benchmark( fill_histogram_tmp, (*input_params, 'elw'), n_repeat=1000, n_warmup=10 )\n\nprint (tau0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sergey's modification on 1 GPU\ninput_params = generate_input(n_rows=pow(10,6),n_cols=99,n_out=10,max_bin=256,nnodes=32,verbose=False)\ntau1 = benchmark( fill_histogram_tmp, (*input_params, 'ser'), n_repeat=100, n_warmup=10 )\nprint (tau1)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:43:09.599773Z","iopub.execute_input":"2023-12-14T11:43:09.600132Z","iopub.status.idle":"2023-12-14T11:43:14.722729Z","shell.execute_reply.started":"2023-12-14T11:43:09.600104Z","shell.execute_reply":"2023-12-14T11:43:14.721795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sergey's modification on 2 GPUs\ninput_params = generate_input(n_rows=pow(10,6),n_cols=99,n_out=10,max_bin=256,nnodes=32,verbose=False)\ntau2 = benchmark( fill_histogram_tmp, (*input_params, 'ser2'), n_repeat=100, n_warmup=10 )\nprint (tau2)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:50:52.593604Z","iopub.execute_input":"2023-12-14T11:50:52.593988Z","iopub.status.idle":"2023-12-14T11:50:58.554925Z","shell.execute_reply.started":"2023-12-14T11:50:52.593957Z","shell.execute_reply":"2023-12-14T11:50:58.554007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of the algorithm for different values of n_rows (time is in microseconds)\ntab_out = Table()\nfname   = 'tab_Anton_Nrows_vs_time.txt'\nfmts    = {'N_rows'       :  '%.2f',\n           'tau_mean_elw' :  '%.6f',\n           'tau_stdev_elw':  '%.6f',\n           'tau_mean_ser' :  '%.6f',\n           'tau_stdev_ser':  '%.6f',\n           'tau_mean_ser2' : '%.6f',\n           'tau_stdev_ser2': '%.6f'}\n\nexponent    = np.arange(2,7.1,0.25)    \nn_rows      = np.array([pow(10,x) for x in exponent],dtype=np.int32)\nn_threads   = 1024\nto_microsec = pow(10,6)\n\nns, means_elw, stdevs_elw, means_ser, stdevs_ser, means_ser2, stdevs_ser2 = [],[],[],[],[],[],[]\nfor n in n_rows:\n    input_params = generate_input(n_rows=n,n_cols=99,n_out=10,max_bin=256,nnodes=32,verbose=False)\n    tau_elw  = benchmark( fill_histogram_tmp, (*input_params, 'elw'),  n_repeat=100, n_warmup=10 )\n    tau_ser  = benchmark( fill_histogram_tmp, (*input_params, 'ser'),  n_repeat=100, n_warmup=10 )\n    tau_ser2 = benchmark( fill_histogram_tmp, (*input_params, 'ser2'), n_repeat=100, n_warmup=10 )\n    \n    mean_elw, stdev_elw = np.average(tau_elw.gpu_times), np.std(tau_elw.gpu_times)\n    mean_ser, stdev_ser = np.average(tau_ser.gpu_times), np.std(tau_ser.gpu_times)\n    mean_ser2, stdev_ser2 = np.average(tau_ser2.gpu_times), np.std(tau_ser2.gpu_times)\n    \n    print ('N_rows = 10^{:.2f}'.format(np.log10(n)))\n    print ('time_elw  = {:.6f}+/-{:.6f} microsec'.format(mean_elw*to_microsec,  stdev_elw*to_microsec))\n    print ('time_ser  = {:.6f}+/-{:.6f} microsec'.format(mean_ser*to_microsec,  stdev_ser*to_microsec))\n    print ('time_ser2 = {:.6f}+/-{:.6f} microsec'.format(mean_ser2*to_microsec, stdev_ser2*to_microsec))\n\n    ns.append( np.log10(n) )\n    means_elw.append(mean_elw*to_microsec)\n    stdevs_elw.append(stdev_elw*to_microsec)\n\n    means_ser.append(mean_ser*to_microsec)\n    stdevs_ser.append(stdev_ser*to_microsec)\n    \n    means_ser2.append(mean_ser2*to_microsec)\n    stdevs_ser2.append(stdev_ser2*to_microsec)\n    \ntab_out['N_rows']    = ns\ntab_out['tau_mean_elw']  = means_elw\ntab_out['tau_stdev_elw'] = stdevs_elw\n\ntab_out['tau_mean_ser']  = means_ser\ntab_out['tau_stdev_ser'] = stdevs_ser\n\ntab_out['tau_mean_ser2']  = means_ser2\ntab_out['tau_stdev_ser2'] = stdevs_ser2\n\ntab_out.write( fname, format='ascii.fixed_width', formats=fmts, bookend=False, delimiter=None, overwrite=True )","metadata":{"execution":{"iopub.status.busy":"2023-12-14T11:59:35.652827Z","iopub.execute_input":"2023-12-14T11:59:35.653606Z","iopub.status.idle":"2023-12-14T12:05:26.730322Z","shell.execute_reply.started":"2023-12-14T11:59:35.653569Z","shell.execute_reply":"2023-12-14T12:05:26.729348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of the algorithm for different values of n_cols (time is in microseconds)\ntab_out = Table()\nfname   = 'tab_Anton_Ncols_vs_time.txt'\nfmts    = {'N_cols'       :  '%d',\n           'tau_mean_elw' :  '%.6f',\n           'tau_stdev_elw':  '%.6f',\n           'tau_mean_ser' :  '%.6f',\n           'tau_stdev_ser':  '%.6f',\n           'tau_mean_ser2' : '%.6f',\n           'tau_stdev_ser2': '%.6f'}\n  \nn_cols      = np.arange(50, 1051, 100)\nn_threads   = 1024\nto_microsec = pow(10,6)\n\nns, means_elw, stdevs_elw, means_ser, stdevs_ser, means_ser2, stdevs_ser2 = [],[],[],[],[],[],[]\nfor n in n_cols:\n    input_params = generate_input(n_rows=pow(10,6),n_cols=n,n_out=10,max_bin=256,nnodes=32,verbose=False)\n    tau_elw  = benchmark( fill_histogram_tmp, (*input_params, 'elw'),  n_repeat=100, n_warmup=10 )\n    tau_ser  = benchmark( fill_histogram_tmp, (*input_params, 'ser'),  n_repeat=100, n_warmup=10 )\n    tau_ser2 = benchmark( fill_histogram_tmp, (*input_params, 'ser2'), n_repeat=100, n_warmup=10 )\n    \n    mean_elw, stdev_elw = np.average(tau_elw.gpu_times), np.std(tau_elw.gpu_times)\n    mean_ser, stdev_ser = np.average(tau_ser.gpu_times), np.std(tau_ser.gpu_times)\n    mean_ser2, stdev_ser2 = np.average(tau_ser2.gpu_times), np.std(tau_ser2.gpu_times)\n    \n    print ('N_cols = {}'.format(n))\n    #print (tau)\n    print ('time_elw = {:.6f}+/-{:.6f} microsec'.format(mean_elw*to_microsec, stdev_elw*to_microsec))\n    print ('time_ser = {:.6f}+/-{:.6f} microsec'.format(mean_ser*to_microsec, stdev_ser*to_microsec))\n    print ('time_ser2 = {:.6f}+/-{:.6f} microsec'.format(mean_ser2*to_microsec, stdev_ser2*to_microsec))\n\n    \n    ns.append( n )\n    means_elw.append(mean_elw*to_microsec)\n    stdevs_elw.append(stdev_elw*to_microsec)\n\n    means_ser.append(mean_ser*to_microsec)\n    stdevs_ser.append(stdev_ser*to_microsec)\n\n    means_ser2.append(mean_ser2*to_microsec)\n    stdevs_ser2.append(stdev_ser2*to_microsec)\n    \ntab_out['N_cols']        = ns\ntab_out['tau_mean_elw']  = means_elw\ntab_out['tau_stdev_elw'] = stdevs_elw\n\ntab_out['tau_mean_ser']  = means_ser\ntab_out['tau_stdev_ser'] = stdevs_ser\n\ntab_out['tau_mean_ser2']  = means_ser2\ntab_out['tau_stdev_ser2'] = stdevs_ser2\n\ntab_out.write( fname, format='ascii.fixed_width', formats=fmts, bookend=False, delimiter=None, overwrite=True )","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:09:05.156824Z","iopub.execute_input":"2023-12-14T12:09:05.157625Z","iopub.status.idle":"2023-12-14T12:28:46.995583Z","shell.execute_reply.started":"2023-12-14T12:09:05.157587Z","shell.execute_reply":"2023-12-14T12:28:46.994578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of the algorithm for different values of n_bins (time is in microseconds)\ntab_out = Table()\nfname   = 'tab_Anton_Nmaxbins_vs_time.txt'\nfmts    = {'N_mbins'      :  '%d',\n           'tau_mean_elw' :  '%.6f',\n           'tau_stdev_elw':  '%.6f',\n           'tau_mean_ser' :  '%.6f',\n           'tau_stdev_ser':  '%.6f',\n           'tau_mean_ser2' : '%.6f',\n           'tau_stdev_ser2': '%.6f'}\n\nn_bins      = np.arange(8,260,8) \nn_threads   = 1024\nto_microsec = pow(10,6)\n\nns, means_elw, stdevs_elw, means_ser, stdevs_ser, means_ser2, stdevs_ser2 = [],[],[],[],[],[],[]\nfor n in n_bins:\n    input_params = generate_input(n_rows=pow(10,6),n_cols=99,n_out=10,max_bin=n,nnodes=32,verbose=False)\n    tau_elw  = benchmark( fill_histogram_tmp, (*input_params, 'elw'),  n_repeat=100, n_warmup=10 )\n    tau_ser  = benchmark( fill_histogram_tmp, (*input_params, 'ser'),  n_repeat=100, n_warmup=10 )\n    tau_ser2 = benchmark( fill_histogram_tmp, (*input_params, 'ser2'), n_repeat=100, n_warmup=10 )\n        \n    mean_elw, stdev_elw = np.average(tau_elw.gpu_times), np.std(tau_elw.gpu_times)\n    mean_ser, stdev_ser = np.average(tau_ser.gpu_times), np.std(tau_ser.gpu_times)\n    mean_ser2, stdev_ser2 = np.average(tau_ser2.gpu_times), np.std(tau_ser2.gpu_times)\n    \n    print ('N_mbins = {}'.format(n))\n    #print (tau)\n    print ('time_elw = {:.6f}+/-{:.6f} microsec'.format(mean_elw*to_microsec, stdev_elw*to_microsec))\n    print ('time_ser = {:.6f}+/-{:.6f} microsec'.format(mean_ser*to_microsec, stdev_ser*to_microsec))\n    print ('time_ser2 = {:.6f}+/-{:.6f} microsec'.format(mean_ser2*to_microsec, stdev_ser2*to_microsec))\n\n    \n    ns.append( n )\n    means_elw.append(mean_elw*to_microsec)\n    stdevs_elw.append(stdev_elw*to_microsec)\n\n    means_ser.append(mean_ser*to_microsec)\n    stdevs_ser.append(stdev_ser*to_microsec)\n\n    means_ser2.append(mean_ser2*to_microsec)\n    stdevs_ser2.append(stdev_ser2*to_microsec)\n    \ntab_out['N_mbins']       = ns\ntab_out['tau_mean_elw']  = means_elw\ntab_out['tau_stdev_elw'] = stdevs_elw\n\ntab_out['tau_mean_ser']  = means_ser\ntab_out['tau_stdev_ser'] = stdevs_ser\n\ntab_out['tau_mean_ser2']  = means_ser2\ntab_out['tau_stdev_ser2'] = stdevs_ser2\n\ntab_out.write( fname, format='ascii.fixed_width', formats=fmts, bookend=False, delimiter=None, overwrite=True )","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:36:31.585722Z","iopub.execute_input":"2023-12-14T12:36:31.586101Z","iopub.status.idle":"2023-12-14T12:44:16.350746Z","shell.execute_reply.started":"2023-12-14T12:36:31.586064Z","shell.execute_reply":"2023-12-14T12:44:16.349867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of the algorithm for different values of n_nodes (time is in microseconds)\ntab_out = Table()\nfname   = 'tab_Anton_Nnodes_vs_time.txt'\nfmts    = {'N_nodes'  :  '%d',\n           'tau_mean_elw' :  '%.6f',\n           'tau_stdev_elw':  '%.6f',\n           'tau_mean_ser' :  '%.6f',\n           'tau_stdev_ser':  '%.6f',\n           'tau_mean_ser2' : '%.6f',\n           'tau_stdev_ser2': '%.6f'}\n  \nn_nodes     = np.arange(8,260,8) \nn_threads   = 1024\nto_microsec = pow(10,6)\n\nns, means_elw, stdevs_elw, means_ser, stdevs_ser, means_ser2, stdevs_ser2 = [],[],[],[],[],[],[]\nfor n in n_nodes:\n    input_params = generate_input(n_rows=pow(10,6),n_cols=99,n_out=10,max_bin=256,nnodes=n,verbose=False)\n    tau_elw  = benchmark( fill_histogram_tmp, (*input_params, 'elw'),  n_repeat=100, n_warmup=10 )\n    tau_ser  = benchmark( fill_histogram_tmp, (*input_params, 'ser'),  n_repeat=100, n_warmup=10 )\n    tau_ser2 = benchmark( fill_histogram_tmp, (*input_params, 'ser2'), n_repeat=100, n_warmup=10 )\n    \n    mean_elw, stdev_elw = np.average(tau_elw.gpu_times), np.std(tau_elw.gpu_times)\n    mean_ser, stdev_ser = np.average(tau_ser.gpu_times), np.std(tau_ser.gpu_times)\n    mean_ser2, stdev_ser2 = np.average(tau_ser2.gpu_times), np.std(tau_ser2.gpu_times)\n        \n    print ('N_nodes = {}'.format(n))\n    #print (tau)\n    print ('time_elw = {:.6f}+/-{:.6f} microsec'.format(mean_elw*to_microsec, stdev_elw*to_microsec))\n    print ('time_ser = {:.6f}+/-{:.6f} microsec'.format(mean_ser*to_microsec, stdev_ser*to_microsec))\n    print ('time_ser2 = {:.6f}+/-{:.6f} microsec'.format(mean_ser2*to_microsec, stdev_ser2*to_microsec))\n\n    ns.append( n )\n    means_elw.append(mean_elw*to_microsec)\n    stdevs_elw.append(stdev_elw*to_microsec)\n\n    means_ser.append(mean_ser*to_microsec)\n    stdevs_ser.append(stdev_ser*to_microsec)\n    \n    means_ser2.append(mean_ser2*to_microsec)\n    stdevs_ser2.append(stdev_ser2*to_microsec)\n    \ntab_out['N_nodes']       = ns\ntab_out['tau_mean_elw']  = means_elw\ntab_out['tau_stdev_elw'] = stdevs_elw\n\ntab_out['tau_mean_ser']  = means_ser\ntab_out['tau_stdev_ser'] = stdevs_ser\n\ntab_out['tau_mean_ser2']  = means_ser2\ntab_out['tau_stdev_ser2'] = stdevs_ser2\n\ntab_out.write( fname, format='ascii.fixed_width', formats=fmts, bookend=False, delimiter=None, overwrite=True )","metadata":{"execution":{"iopub.status.busy":"2023-12-14T12:45:11.841366Z","iopub.execute_input":"2023-12-14T12:45:11.842343Z","iopub.status.idle":"2023-12-14T13:03:56.060021Z","shell.execute_reply.started":"2023-12-14T12:45:11.842308Z","shell.execute_reply":"2023-12-14T13:03:56.058960Z"},"trusted":true},"execution_count":null,"outputs":[]}]}